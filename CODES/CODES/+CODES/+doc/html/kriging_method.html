
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>CODES / fit / kriging (methods)</title><meta name="generator" content="MATLAB 8.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2015-06-23"><meta name="DC.source" content="kriging_method.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1><a href="CODES.html">CODES</a> / <a href="fit_main.html">fit</a> / kriging (methods)</h1><!--introduction--><p><i>Methods of the class</i> <tt>kriging</tt></p><p>
  <style type="text/css">
    span.string{color:#A020F0;font-family:monospace;}
    p{text-align: justify;}
  </style>
</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">eval</a></li><li><a href="#4">eval_var</a></li><li><a href="#6">eval_all</a></li><li><a href="#8">P_pos</a></li><li><a href="#10">class</a></li><li><a href="#12">eval_class</a></li><li><a href="#14">scale</a></li><li><a href="#16">unscale</a></li><li><a href="#18">scale_y</a></li><li><a href="#20">unscale_y</a></li><li><a href="#23">add</a></li><li><a href="#25">mse</a></li><li><a href="#27">rmse</a></li><li><a href="#29">nmse</a></li><li><a href="#31">rmae</a></li><li><a href="#33">r2</a></li><li><a href="#35">me</a></li><li><a href="#37">auc</a></li><li><a href="#39">loo</a></li><li><a href="#41">cv</a></li><li><a href="#43">class_change</a></li><li><a href="#45">plot</a></li><li><a href="#47">isoplot</a></li><li><a href="#50">redLogLH</a></li></ul></div><p><a id="eval"></a></p><h2>eval<a name="2"></a></h2><p>Evaluate mean prediction of new samples <tt>x</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>y_hat=kr.eval(x)</tt> return the Kriging values <tt>y_hat</tt> of the samples <tt>x</tt>.</li><li><tt>[y_hat,grad]=kr.eval(x)</tt> return the gradients <tt>grad</tt> at <tt>x</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
[y_hat,grad]=kr.eval([10 0.5;5 0.6;14 0.8]);
CODES.common.disp_matrix([y_hat grad],[],{<span class="string">'Y'</span>,<span class="string">'grad1'</span>,<span class="string">'grad2'</span>})
</pre><pre class="codeoutput">         Y       grad1     grad2
  0.577501  -0.0241364  0.484064
  0.918653  -0.0579477   2.09191
  0.562664  -0.0282549  0.155311
</pre><p><h3>See also</h3><a href=#eval_var>eval_var</a> <a
href=#eval_all>eval_all</a> <a href=#p_pos>P_pos</a></p><p><a id="eval_var"></a></p><h2>eval_var<a name="4"></a></h2><p>Evaluate predicted variance at new samples <tt>x</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>var_hat=kr.eval_var(x)</tt> return the Kriging variance <tt>var_hat</tt> of the samples <tt>x</tt>.</li><li><tt>[var_hat,grad]=kr.eval_var(x)</tt> return the gradients <tt>grad</tt> at <tt>x</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
[y_hat,grad]=kr.eval_var([10 0.5;5 0.6;14 0.8]);
CODES.common.disp_matrix([y_hat grad],[],{<span class="string">'Y'</span>,<span class="string">'grad1'</span>,<span class="string">'grad2'</span>})
</pre><pre class="codeoutput">        Y       grad1       grad2
    2.244  0.00374117  -0.0750312
  2.07474   0.0485198    -1.75156
  2.24607  0.00354417  -0.0196841
</pre><p><h3>See also</h3><a href=#eval>eval</a> <a
href=#eval_all>eval_all</a> <a href=#p_pos>P_pos</a></p><p><a id="eval_all"></a></p><h2>eval_all<a name="6"></a></h2><p>Evaluate predicted mean and variance at new samples <tt>x</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>y_hat=kr.eval_var(x)</tt> return the Kriging variance <tt>var_hat</tt> of the samples <tt>x</tt>.</li><li><tt>[y_hat,var_hat]=kr.eval_var(x)</tt> return the Kriging variance <tt>var_hat</tt> of the samples <tt>x</tt>.</li><li><tt>[y_hat,var_hat,grad_y]=kr.eval_var(x)</tt> return the gradients of the mean <tt>grad_y</tt> at <tt>x</tt>.</li><li><tt>[y_hat,var_hat,grad_y,grad_var]=kr.eval_var(x)</tt> return the gradients of the variance <tt>grad_var</tt> at <tt>x</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
[y_hat,var_hat,grad_y,grad_var]=kr.eval_all([10 0.5;5 0.6;14 0.8]);
CODES.common.disp_matrix([y_hat var_hat grad_y grad_var],[],<span class="keyword">...</span>
    {<span class="string">'Y'</span>,<span class="string">'Var'</span>,<span class="string">'Y_grad1'</span>,<span class="string">'Y_grad2'</span>,<span class="string">'Var_grad1'</span>,<span class="string">'Var_grad2'</span>})
</pre><pre class="codeoutput">         Y      Var     Y_grad1   Y_grad2   Var_grad1   Var_grad2
  0.577501    2.244  -0.0241364  0.484064  0.00374117  -0.0750312
  0.918653  2.07474  -0.0579477   2.09191   0.0485198    -1.75156
  0.562664  2.24607  -0.0282549  0.155311  0.00354417  -0.0196841
</pre><p><h3>See also</h3><a href=#eval>eval</a> <a
href=#eval_var>eval_var</a> <a href=#p_pos>P_pos</a></p><p><a id="p_pos"></a></p><h2>P_pos<a name="8"></a></h2><p>Compute the probability of the kriging prediction to be higher than a threshold.</p><p><h3>Syntax</h3></p><div><ul><li><tt>p=kr.eval_var(x)</tt> return the probability <tt>p</tt> of the samples <tt>x</tt> to be higher than 0.</li><li><tt>p=kr.eval_var(x,th)</tt> return the probability <tt>p</tt> of the samples <tt>x</tt> to be higher than <tt>th</tt>.</li><li><tt>[p,grad]=kr.eval_var(...)</tt> return the gradient <tt>grad</tt> of probability <tt>p</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
[p,grad]=kr.P_pos([10 0.5;5 0.6;14 0.8],0.25);
CODES.common.disp_matrix([p grad],[],{<span class="string">'p'</span>,<span class="string">'grad1'</span>,<span class="string">'grad2'</span>})
</pre><pre class="codeoutput">         p        grad1      grad2
  0.586529  -0.00634712   0.127294
  0.678753   -0.0163545   0.590396
   0.58263  -0.00742364  0.0408098
</pre><p><a id="class"></a></p><h2>class<a name="10"></a></h2><p>Provides the sign of input <tt>y</tt>, different than MATLAB sign function for <tt>y=0</tt>.</p><p><h3>Syntax</h3></p><div><ul><li><tt>lab=kr.class(y)</tt> computes labels <tt>lab</tt> for function values <tt>y</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">svm=CODES.fit.svm([1;2],[1;-1]);
y=[1;-1;-2;3;0];
lab=svm.class(y);
disp([[<span class="string">'   y : '</span>;<span class="string">' lab : '</span>;<span class="string">'sign : '</span>] num2str([y';lab';sign(y')],<span class="string">'%1.3f  '</span>)])
</pre><pre class="codeoutput">   y : 1.000  -1.000  -2.000   3.000   0.000
 lab : 1.000  -1.000  -1.000   1.000  -1.000
sign : 1.000  -1.000  -1.000   1.000   0.000
</pre><p><h3>See also</h3><a href=#eval_class>eval_class</a></p><p><a id="eval_class"></a></p><h2>eval_class<a name="12"></a></h2><p>Evaluate class of new samples <tt>x</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>lab=kr.eval_class(x)</tt> computes the labels <tt>lab</tt> of the input samples <tt>x</tt>.</li><li><tt>[lab,y_hat]</tt>=kr.eval_class(x) also returns predicted function values <tt>y_hat</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1;2],[1;-1]);
x=[0;1;2;3];
[lab,y_hat]=kr.eval_class(x);
disp([[<span class="string">'y_hat : '</span>;<span class="string">'  lab : '</span>] num2str([y_hat';lab'],<span class="string">'%1.3f  '</span>)])
</pre><pre class="codeoutput">y_hat : 0.000   1.000  -1.000  -0.000
  lab : 1.000   1.000  -1.000  -1.000
</pre><p><h3>See also</h3><a href=#class>class</a></p><p><a id="scale"></a></p><h2>scale<a name="14"></a></h2><p>Perform scaling of samples <tt>x_unsc</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>x_sc=kr.scale(x_unsc)</tt> scales <tt>x_unsc</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[1;-1]);
x_unsc=[1 1;10 1.5;20 2];
x_sc=kr.scale(x_unsc);
disp(<span class="string">'        Unscaled             Scaled'</span>)
disp([x_unsc x_sc])
</pre><pre class="codeoutput">        Unscaled             Scaled
    1.0000    1.0000         0         0
   10.0000    1.5000    0.4737    0.5000
   20.0000    2.0000    1.0000    1.0000

</pre><p><h3>See also</h3><a href=#unscale>unscale</a></p><p><a id="unscale"></a></p><h2>unscale<a name="16"></a></h2><p>Perform unscaling of samples <tt>x_sc</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>x_unsc=kr.unscale(x_sc)</tt> unscales <tt>x_sc</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[1;-1]);
x_sc=[0 0;0.4737 -1;1 1];
x_unsc=kr.unscale(x_sc);
disp(<span class="string">'         Scaled             Unscaled'</span>)
disp([x_sc x_unsc])
</pre><pre class="codeoutput">         Scaled             Unscaled
         0         0    1.0000    1.0000
    0.4737   -1.0000   10.0003         0
    1.0000    1.0000   20.0000    2.0000

</pre><p><h3>See also</h3><a href=#scale>scale</a></p><p><a id="scale_y"></a></p><h2>scale_y<a name="18"></a></h2><p>Perform scaling of function values <tt>y_unsc</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>y_sc=kr.scale_y(y_unsc)</tt> scales <tt>y_unsc</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[1;-3]);
y_unsc=[0.5;1;2];
y_sc=kr.scale_y(y_unsc);
CODES.common.disp_matrix([y_unsc y_sc],[],{<span class="string">'Unscaled'</span>,<span class="string">'Scaled'</span>})
</pre><pre class="codeoutput">  Unscaled  Scaled
       0.5   0.875
         1       1
         2    1.25
</pre><p><h3>See also</h3><a href=#unscale_y>unscale_y</a></p><p><a id="unscale_y"></a></p><h2>unscale_y<a name="20"></a></h2><p>Perform unscaling of function values <tt>y_sc</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>y_unsc=kr.unscale_y(y_sc)</tt> unscales <tt>y_sc</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[1;-1]);
y_sc=[0;0.25;0.75];
y_unsc=kr.unscale_y(y_sc);
CODES.common.disp_matrix([y_unsc y_sc],[],{<span class="string">'Unscaled'</span>,<span class="string">'Scaled'</span>})
</pre><pre class="codeoutput">  Unscaled  Scaled
        -1       0
      -0.5    0.25
       0.5    0.75
</pre><p><h3>See also</h3><a href=#scale>scale</a></p><p><a id="add"></a></p><h2>add<a name="23"></a></h2><p>Retrain <tt>kr</tt> after adding a new sample <tt>(x,y)</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>kr=kr.add(x,y)</tt> adds a new sample <tt>x</tt> with function value <tt>y</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1;2],[1;-1]);
disp([<span class="string">'Predicted class at x=1.4, '</span> num2str(kr.eval_class(1.4))])
kr=kr.add(1.5,-1);
disp([<span class="string">'Updated predicted class at x=1.4, '</span> num2str(kr.eval_class(1.4))])
</pre><pre class="codeoutput">Predicted class at x=1.4, 1
Updated predicted class at x=1.4, -1
</pre><p><a id="mse"></a></p><h2>mse<a name="25"></a></h2><p>Compute the Mean Square Error (MSE) for <tt>(x,y)</tt> (not for classification)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.mse(x,y)</tt> computes the MSE for the samples <tt>(x,y)</tt>.</li></ul></div><p><h3>Description</h3></p><p>For a representative (sample,label) <img src="kriging_method_eq05145133829109721124.png" alt="$(\mathbf{x},\mathbf{y})$"> of the domain of interest and predicted values <img src="kriging_method_eq02834496625363855977.png" alt="$\tilde{\mathbf{y}}$">, the Mean Square Error is defined as:</p><p><img src="kriging_method_eq12778036165001245747.png" alt="$$mse=\frac{1}{n}\sum_{i=1}^n\left(y^{(i)}-\tilde{y}^{(i)}\right)^2$$"></p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.mse(x_t,y_t);
disp(<span class="string">'Mean Square Error:'</span>)
disp(err)
</pre><pre class="codeoutput">Mean Square Error:
    4.8282

</pre><p><h3>See also</h3><a href=#auc>auc</a> | <a
href=#me>me</a> | <a href=#rmse>rmse</a> | <a
href=#rmae>rmae</a> | <a href=#r2>r2</a> | <a
href=#cv>cv</a> | <a href=#loo>loo</a></p><p><a id="rmse"></a></p><h2>rmse<a name="27"></a></h2><p>Compute the Root Mean Square Error (RMSE) for <tt>(x,y)</tt> (not for classification)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.rmse(x,y)</tt> computes the RMSE for the samples <tt>(x,y)</tt>.</li></ul></div><p><h3>Description</h3></p><p>For a representative (sample,label) <img src="kriging_method_eq05145133829109721124.png" alt="$(\mathbf{x},\mathbf{y})$"> of the domain of interest and predicted values <img src="kriging_method_eq02834496625363855977.png" alt="$\tilde{\mathbf{y}}$">, the Root Mean Square Error is defined as:</p><p><img src="kriging_method_eq13280307167645970570.png" alt="$$rmse=\sqrt{\frac{1}{n}\sum_{i=1}^n\left(y^{(i)}-\tilde{y}^{(i)}\right)^2}$$"></p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.rmse(x_t,y_t);
disp(<span class="string">'Root Mean Square Error:'</span>)
disp(err)
</pre><pre class="codeoutput">Root Mean Square Error:
    2.1973

</pre><p><h3>See also</h3><a href=#auc>auc</a> | <a
href=#me>me</a> | <a href=#mse>mse</a> | <a
href=#rmae>rmae</a> | <a href=#r2>r2</a></p><p><a id="nmse"></a></p><h2>nmse<a name="29"></a></h2><p>Compute the Normalized Mean Square Error (NMSE) (%) for <tt>(x,y)</tt> (not for classification)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.nmse(x,y)</tt> computes the NMSE for the samples <tt>(x,y)</tt>.</li></ul></div><p><h3>Description</h3></p><p>For a representative (sample,label) <img src="kriging_method_eq05145133829109721124.png" alt="$(\mathbf{x},\mathbf{y})$"> of the domain of interest and predicted values <img src="kriging_method_eq02834496625363855977.png" alt="$\tilde{\mathbf{y}}$">, the Normalized Mean Square Error is defined as:</p><p><img src="kriging_method_eq00747299681892825876.png" alt="$$nmse=\frac{\sum_{i=1}^n\left(y^{(i)}-\tilde{y}^{(i)}\right)^2}{\sum_{i=1}^n\left(y^{(i)}-\bar{\mathbf{y}}\right)^2}$$"></p><p>where <img src="kriging_method_eq13465139074281208873.png" alt="$\bar{\mathbf{y}}$"> is the average of the training values <img src="kriging_method_eq01759449390978539805.png" alt="$\mathbf{y}$">.</p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.nmse(x_t,y_t);
disp(<span class="string">'Normalized Mean Square Error:'</span>)
disp([num2str(err,<span class="string">'%5.2f'</span>) <span class="string">' %'</span>])
</pre><pre class="codeoutput">Normalized Mean Square Error:
35.30 %
</pre><p><h3>See also</h3><a href=#auc>auc</a> | <a
href=#me>me</a> | <a href=#mse>mse</a> | <a href=#rmse>rmse</a> | <a
href=#rmae>rmae</a> | <a href=#r2>r2</a></p><p><a id="rmae"></a></p><h2>rmae<a name="31"></a></h2><p>Compute the Relative Maximum Absolute Error (RMAE) for <tt>(x,y)</tt> (not for classification)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.rmae(x,y)</tt> computes the RMAE for the samples <tt>(x,y)</tt>.</li></ul></div><p><h3>Description</h3></p><p>For a representative (sample,label) <img src="kriging_method_eq05145133829109721124.png" alt="$(\mathbf{x},\mathbf{y})$"> of the domain of interest and predicted values <img src="kriging_method_eq02834496625363855977.png" alt="$\tilde{\mathbf{y}}$">, the Relative Maximum Absolute Error is defined as:</p><p><img src="kriging_method_eq15144645622141433903.png" alt="$$rmae=\frac{\mathop{\max}\limits_{i}\ \left|y^{(i)}-\tilde{y}^{(i)}\right|}{\sigma_\mathbf{y}}$$"></p><p>where <img src="kriging_method_eq06650964498218025409.png" alt="$\sigma_\mathbf{y}$"> is the standard deviation of the training values <img src="kriging_method_eq01759449390978539805.png" alt="$\mathbf{y}$">:</p><p><img src="kriging_method_eq13768450941034233838.png" alt="$$\sigma_\mathbf{y}=\sqrt{\frac{1}{n-1}\sum_{i=1}^n\left(y^{(i)}-\bar{\mathbf{y}}\right)^2}$$"></p><p>where <img src="kriging_method_eq13465139074281208873.png" alt="$\bar{\mathbf{y}}$"> is the average of the training values <img src="kriging_method_eq01759449390978539805.png" alt="$\mathbf{y}$">.</p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.rmae(x_t,y_t);
disp(<span class="string">'Relative Maximum Absolute Error:'</span>)
disp(err)
</pre><pre class="codeoutput">Relative Maximum Absolute Error:
    1.6722

</pre><p><h3>See also</h3><a href=#auc>auc</a> | <a
href=#me>me</a> | <a href=#mse>mse</a> | <a href=#rmse>rmse</a> | <a
href=#r2>r2</a></p><p><a id="r2"></a></p><h2>r2<a name="33"></a></h2><p>Compute the coefficient of determination (R squared) for <tt>(x,y)</tt> (not for classification)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.r2(x,y)</tt> computes the R squared for the samples <tt>(x,y)</tt></li><li><tt>[stat,TSS]=kr.r2(x,y)</tt> return the Total Sum of Squares <tt>TSS</tt></li><li><tt>[stat,TSS,RSS]=kr.r2(x,y)</tt> returns the Residual Sum of Squares <tt>RSS</tt></li></ul></div><p><h3>Description</h3></p><p>For a representative (sample,label) <img src="kriging_method_eq05145133829109721124.png" alt="$(\mathbf{x},\mathbf{y})$"> of the domain of interest and predicted values <img src="kriging_method_eq02834496625363855977.png" alt="$\tilde{\mathbf{y}}$">, the coefficient of determination is defined as:</p><p><img src="kriging_method_eq15689890389455703583.png" alt="$$r2=1-\frac{RSS}{TSS}$$"></p><p>where:</p><p><img src="kriging_method_eq12275425659777288021.png" alt="$$RSS=\sum_{i=1}^n\left(y^{(i)}-\tilde{y}^{(i)}\right)^2$$"></p><p><img src="kriging_method_eq13876040288998166071.png" alt="$$TSS=\sum_{i=1}^n\left(\tilde{y}^{(i)}-\bar{\mathbf{y}}\right)^2$$"></p><p>where <img src="kriging_method_eq13465139074281208873.png" alt="$\bar{\mathbf{y}}$"> is the average of the training values <img src="kriging_method_eq01759449390978539805.png" alt="$\mathbf{y}$">.</p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.r2(x_t,y_t);
disp(<span class="string">'Coefficient of determination:'</span>)
disp(err)
</pre><pre class="codeoutput">Coefficient of determination:
    0.4391

</pre><p><h3>See also</h3><a href=#auc>auc</a> | <a
href=#me>me</a> | <a href=#mse>mse</a> | <a href=#rmse>rmse</a> | <a
href=#rmae>rmae</a></p><p><a id="me"></a></p><h2>me<a name="35"></a></h2><p>Compute the Misclassification Error (ME) for <tt>(x,y)</tt> (%)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.me(x,y)</tt> compute the me for <tt>(x,y)</tt></li><li><tt>stat=kr.me(x,y,use_balanced)</tt> returns Balanced Misclassification Error (BME) if use_balanced is set to true</li></ul></div><p><h3>Description</h3></p><p>For a representative (sample,label) <img src="kriging_method_eq05145133829109721124.png" alt="$(\mathbf{x},\mathbf{y})$"> of the domain of interest and predicted labels <img src="kriging_method_eq02834496625363855977.png" alt="$\tilde{\mathbf{y}}$">, the classification error is defined as:</p><p><img src="kriging_method_eq13052294279100284555.png" alt="$$err_{class}=\frac{1}{n}\sum_{i=1}^n\mathcal{I}_{y^{(i)}\neq\tilde{y}^{(i)}}$$"></p><p>On the other hand, the balanced classification error is defined as:</p><p><img src="kriging_method_eq07423943349070457344.png" alt="$$err_{class}^{bal}=\frac{1}{n}\sum_{i=1}^n\left[w_p\mathcal{I}_{y^{(i)}=+1}\mathcal{I}_{y^{(i)}\neq\tilde{y}^{(i)}}+w_m\mathcal{I}_{y^{(i)}=-1}\mathcal{I}_{y^{(i)}\neq\tilde{y}^{(i)}}\right]$$"></p><p>where <img src="kriging_method_eq13615530149322219580.png" alt="$w_p$"> and <img src="kriging_method_eq10453351923203779427.png" alt="$w_m$"> are weights computed based on training samples such that:</p><p><img src="kriging_method_eq13443144576051923421.png" alt="$$w_p=\frac{n}{2n_+}\quad;\quad w_m=\frac{n}{2n_-}$$"></p><p>where <img src="kriging_method_eq08984225997457563733.png" alt="$n$"> is the total number of samples and <img src="kriging_method_eq05393789287629341632.png" alt="$n_+$"> (resp. <img src="kriging_method_eq05547798297214684078.png" alt="$n_-$">) is the total number of positive (resp. negative) samples. This weights satisfy a set of condition:</p><div><ul><li><img src="kriging_method_eq02918049525015467508.png" alt="$err_{class}^{bal}=0.5$"> if all positive or all negative samples are misclassified;</li><li><img src="kriging_method_eq13329342247535284504.png" alt="$err_{class}^{bal}=0$"> if all samples are properly classified;</li><li><img src="kriging_method_eq11515776470572805826.png" alt="$err_{class}^{bal}=1$"> if all samples are misclassified;</li><li><img src="kriging_method_eq13203487386651983277.png" alt="$w_p=w_m=1\textrm{ if }n_+=n_-=\frac{n}{2}\quad\Rightarrow\quad err_{class}^{bal}=err_{class}$">.</li></ul></div><p>This function is typically used to validate meta-models on an independent validation set as in <a href="#ref_peng">Jiang and Missoum (2014)</a>.</p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x-4;
x=[2;8];y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.me(x_t,y_t);
bal_err=kr.me(x_t,y_t,true);
disp(<span class="string">'On evenly balanced training set, standard and balanced prediction error return same values'</span>)
disp([err bal_err])
x=[2;5;8];y=f(x);
kr=CODES.fit.kriging(x,y);
err=kr.me(x_t,y_t);
bal_err=kr.me(x_t,y_t,true);
disp(<span class="string">'On unevenly balanced training set, standard and balanced prediction error return different values'</span>)
disp([err bal_err])
</pre><pre class="codeoutput">On evenly balanced training set, standard and balanced prediction error return same values
    4.4600    4.4600

On unevenly balanced training set, standard and balanced prediction error return different values
    0.0500    0.0375

</pre><p><h3>See also</h3><a href=#auc>auc</a> | <a href=#mse>mse</a> | <a
href=#rmse>rmse</a> | <a href=#cv>cv</a> | <a href=#loo>loo</a></p><p><a id="auc"></a></p><h2>auc<a name="37"></a></h2><p>Returns the Area Under the Curve (AUC) for (x,y) (%)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=CODES.fit.kr.auc(x,y)</tt> return the AUC <tt>stat</tt> for the samples <tt>(x,y)</tt></li><li><tt>stat=CODES.fit.kr.auc(x,y,ROC)</tt> plot the ROC curves if <tt>ROC|is set to |true</tt></li><li><tt>[stat,FP,TP]=CODES.fit.kr.auc(...)</tt> returns the false positive rate <tt>FP</tt> and the true positive rate <tt>TP</tt></li></ul></div><p><h3>Description</h3></p><p>A receiver operating characteristic (ROC) curve <a href="#ref_metz">Metz (1978)</a> is a graphical representation of the relation between true and false positive predictions for a binary classifier. It uses all possible decision thresholds from the prediction. In the case of kr classification, thresholds are defined by the kr values. More specifically, for each threshold a True Positive Rate:</p><p><img src="kriging_method_eq14040664573123928381.png" alt="$$TPR=\frac{TP}{TP+FN}$$"></p><p>and a False Positive Rate:</p><p><img src="kriging_method_eq17014328496336607052.png" alt="$$FPR=\frac{FP}{FP+TN}$$"></p><p>are calculated. <img src="kriging_method_eq03608697195697531668.png" alt="$TP$"> and <img src="kriging_method_eq13257602763055173868.png" alt="$TN$"> are the number of true positive and true negative predictions while <img src="kriging_method_eq10702124910593566595.png" alt="$FP$"> and <img src="kriging_method_eq16504565153769396011.png" alt="$FN$"> are the number of false positive and false negative predictions, respectively. The ROC curve represents <img src="kriging_method_eq06779603246604757794.png" alt="$TPR$"> as a function of <img src="kriging_method_eq09660471352378676259.png" alt="$FPR$">.</p><p>Once the ROC curve is constructed, the area under the ROC curve (AUC) can be calculated and used as a validation metric. A perfect classifier will have an AUC equal to one. An AUC value of 0.5 indicates no discriminative ability.</p><p><img src='rocexample.png' width=500></p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x(:,2)-sin(10*x(:,1))/4-0.5;
x=CODES.sampling.cvt(30,2);y=f(x);
kr=CODES.fit.kriging(x,y);
auc_val=kr.auc(kr.X,kr.Y);
x_t=rand(1000,2);
y_t=f(x_t);
auc_new=kr.auc(x_t,y_t);
disp([<span class="string">'AUC value over training set : '</span> num2str(auc_val,<span class="string">'%7.3f'</span>)])
disp([<span class="string">' AUC value over testing set : '</span> num2str(auc_new,<span class="string">'%7.3f'</span>)])
</pre><pre class="codeoutput">AUC value over training set : 100.000
 AUC value over testing set : 99.999
</pre><p><h3>See also</h3><a href=#me>me</a> | <a
href=#mse>mse</a> | <a href=#rmse>rmse</a> | <a href=#cv>cv</a></p><p><a id="loo"></a></p><h2>loo<a name="39"></a></h2><p>Returns the Leave One Out (LOO) error (%)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.loo</tt> return the loo error <tt>stat</tt></li><li><tt>stat=kr.loo(param,value)</tt> use set of parameters <tt>param</tt> and values <tt>value</tt> (<i>c.f.</i>, <a href="#params_loo">parameter table</a>)</li></ul></div><p><h3>Description</h3></p><p>Define <img src="kriging_method_eq11320232394588775230.png" alt="$\tilde{l}^{(i)}$"> the <img src="kriging_method_eq11690404991072700141.png" alt="$i\textrm{\textsuperscript{th}}$"> predicted label and <img src="kriging_method_eq02500675203451732306.png" alt="$\tilde{l}_{-j}^{(i)}$"> the <img src="kriging_method_eq11690404991072700141.png" alt="$i\textrm{\textsuperscript{th}}$"> predicted label using the kr trained without the <img src="kriging_method_eq17037864883541505819.png" alt="$j\textrm{\textsuperscript{th}}$"> sample. The Leave One Out (LOO) error is defined as:</p><p><img src="kriging_method_eq13707537903014975383.png" alt="$$err_{loo}=\frac{1}{n}\sum_{i=1}^n\mathcal{I}_{\tilde{l}^{(i)}\neq\tilde{l}_{-i}^{(i)}}$$"></p><p>On the other hand, the balanced LOO error is defined as:</p><p><img src="kriging_method_eq01846700469269783418.png" alt="$$err_{loo}^{bal}=\frac{1}{n}\sum_{i=1}^n\left[w_p\mathcal{I}_{l^{(i)}=+1}\mathcal{I}_{\tilde{l}^{(i)}\neq\tilde{l}_{-i}^{(i)}}+w_m\mathcal{I}_{l^{(i)}=-1}\mathcal{I}_{\tilde{l}^{(i)}\neq\tilde{l}_{-i}^{(i)}}\right]$$"></p><p><img src="kriging_method_eq13615530149322219580.png" alt="$w_p$"> and <img src="kriging_method_eq10453351923203779427.png" alt="$w_m$"> are weights computed based on training samples such that:</p><p><img src="kriging_method_eq13443144576051923421.png" alt="$$w_p=\frac{n}{2n_+}\quad;\quad w_m=\frac{n}{2n_-}$$"></p><p>where <img src="kriging_method_eq08984225997457563733.png" alt="$n$"> is the total number of samples and <img src="kriging_method_eq05393789287629341632.png" alt="$n_+$"> (resp. <img src="kriging_method_eq05547798297214684078.png" alt="$n_-$">) is the total number of positive (resp. negative) samples. This weights satisfy a set of condition:</p><div><ul><li><img src="kriging_method_eq09941698488943797337.png" alt="$err_{loo}^{bal}=0.5$"> if all positive or all negative samples are misclassified;</li><li><img src="kriging_method_eq00666071664298318219.png" alt="$err_{loo}^{bal}=0$"> if no samples are misclassified;</li><li><img src="kriging_method_eq00071328142210285656.png" alt="$err_{loo}^{bal}=1$"> if all samples are miscalssified;</li><li><img src="kriging_method_eq14481380738585057513.png" alt="$w_p=w_m=1\textrm{ if }n_+=n_-=\frac{n}{2}\quad\Rightarrow\quad err_{loo}^{bal}=err_{loo}$">.</li></ul></div><p><h3>Parameters</h3>
<table id="params_loo" style="border: none;width=100%">
  <tr>
    <th><tt>param</tt></th>
    <th><tt>value</tt></th>
    <th>Description</th>
  </tr>
  <tr>
    <td><span class="string">'use_balanced'</span></td>
    <td>logical, {<tt>false</tt>}</td>
    <td>Only for Misclassification Error, uses Balanced Misclassification Error if <tt> true></tt></td>
  </tr>
  <tr>
    <td><span class="string">'metric'</span></td>
    <td>{<span class="string">'me'</span>}, <span class="string">'mse'</span></td>
    <td>Metric on which LOO procedure is applied, Misclassification Error (<span class="string">'me'</span>) or Mean Square Error (<span class="string">'mse'</span>)</td>
  </tr>
</table></p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x(:,2)-sin(10*x(:,1))/4-0.5;
x=CODES.sampling.cvt(6,2);y=f(x);
kr=CODES.fit.kriging(x,y);
loo_err=kr.loo;
bal_loo_err=kr.loo(<span class="string">'use_balanced'</span>,true);
disp(<span class="string">'On evenly balanced training set, standard and balanced loo error return same values'</span>)
disp([loo_err bal_loo_err])
x=CODES.sampling.cvt(5,2);y=f(x);
kr=CODES.fit.kriging(x,y);
loo_err=kr.loo;
bal_loo_err=kr.loo(<span class="string">'use_balanced'</span>,true);
disp(<span class="string">'On unevenly balanced training set, standard and balanced loo error return different values'</span>)
disp([loo_err bal_loo_err])
</pre><pre class="codeoutput">On evenly balanced training set, standard and balanced loo error return same values
    50    50

On unevenly balanced training set, standard and balanced loo error return different values
    40    50

</pre><p><h3>See also</h3><a href=#cv>cv</a> | <a href=#auc>auc</a> | <a
href=#me>me</a> | <a href=#mse>mse</a></p><p><a id="cv"></a></p><h2>cv<a name="41"></a></h2><p>Returns the Cross Validation (CV) error over 10 folds (%)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.cv</tt> return the cv error <tt>stat</tt></li><li><tt>stat=kr.cv(param,value)</tt> use set of parameters <tt>param</tt> and values <tt>value</tt> (<i>c.f.</i>, <a href="#params_cv">parameter table</a>)</li></ul></div><p><h3>Description</h3></p><p>This function follow the same outline as the <a href="#loo"><tt>loo</tt></a> but uses a 10 fold CV procedure instead of an <img src="kriging_method_eq08984225997457563733.png" alt="$n$"> fold one. Therefore, the <tt>cv</tt> and <tt>loo</tt> are the same for <img src="kriging_method_eq08648463619276793002.png" alt="$n\leq10$"> and <tt>cv</tt> returns an estimate of <tt>loo</tt> that is faster to compute for <img src="kriging_method_eq16519651398025082573.png" alt="$n&gt;10$">.</p><p><h3>Parameters</h3>
<table id="params_cv" style="border: none;width=100%">
  <tr>
    <th><tt>param</tt></th>
    <th><tt>value</tt></th>
    <th>Description</th>
  </tr>
  <tr>
    <td><span class="string">'use_balanced'</span></td>
    <td>logical, {<tt>false</tt>}</td>
    <td>Only for Misclassification Error, uses Balanced Misclassification Error if <tt> true></tt></td>
  </tr>
  <tr>
    <td><span class="string">'metric'</span></td>
    <td><span class="string">'auc'</span>, {<span class="string">'me'</span>}, <span class="string">'mse'</span></td>
    <td>Metric on which CV procedure is applied: Area Under the Curve (<span class="string">'auc'</span>), Misclassification Error (<span class="string">'me'</span>) or Mean Square Error (<span class="string">'mse'</span>)</td>
  </tr>
</table></p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x(:,1)-0.5;
x=CODES.sampling.cvt(30,2);y=f(x);
kr=CODES.fit.kriging(x,y);
rng(0); <span class="comment">% To ensure same CV folds</span>
cv_err=kr.cv;
rng(0);
bal_cv_err=kr.cv(<span class="string">'use_balanced'</span>,true);
disp(<span class="string">'On evenly balanced training set, standard and balanced cv error return same values'</span>)
disp([cv_err bal_cv_err])
x=CODES.sampling.cvt(31,2);y=f(x);
kr=CODES.fit.kriging(x,y);
rng(0);
cv_err=kr.cv;
rng(0);
bal_cv_err=kr.cv(<span class="string">'use_balanced'</span>,true);
disp(<span class="string">'On unevenly balanced training set, standard and balanced cv error return different values'</span>)
disp([cv_err bal_cv_err])
</pre><pre class="codeoutput">On evenly balanced training set, standard and balanced cv error return same values
   50.0000   50.0000

On unevenly balanced training set, standard and balanced cv error return different values
   48.3333   49.9444

</pre><p><h3>See also</h3><a href=#loo>loo</a> | <a href=#auc>auc</a> | <a
href=#me>me</a> | <a href=#mse>mse</a></p><p><a id="class_change"></a></p><h2>class_change<a name="43"></a></h2><p>Compute the change of class between two meta-models over a sample <tt>x</tt> (%)</p><p><h3>Syntax</h3></p><div><ul><li><tt>stat=kr.class_change(kr_old,x)</tt> compute the change of class of the sample <tt>x</tt> from meta-model <tt>kr_old</tt> to meta-model <tt>kr</tt></li></ul></div><p><h3>Description</h3></p><p>This metric was used in <a href="#ref_bas">Basudhar and Missoum (2008)</a> as convergence metric and is defined as:</p><p><img src="kriging_method_eq07056039919612685248.png" alt="$$class_{change}=\frac{1}{n_c}\sum_{i=1}^{n_c}\mathcal{I}_{\hat{y}_c^{(i)}\neq\tilde{y}_c^{(i)}}$$"></p><p>where <img src="kriging_method_eq15574468723431895569.png" alt="$n_c$"> is the number of convergence samples and <img src="kriging_method_eq17336806818372065967.png" alt="$\hat{y}_c^{(i)}$"> (resp. <img src="kriging_method_eq14456627300622895063.png" alt="$\tilde{y}_c^{(i)}$">) is the <img src="kriging_method_eq11690404991072700141.png" alt="$i\textrm{\textsuperscript{th}}$"> convergence predicted label using <tt>kr_old</tt> (resp. <tt>kr</tt>).</p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x-4;
x=[2;8];y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.me(x_t,y_t);
kr_new=kr.add(5,f(5));
err_new=kr_new.me(x_t,y_t);
class_change=kr_new.class_change(kr,x_t);
disp([<span class="string">'Absolute change in prediction error : '</span> num2str(abs(err_new-err))])
disp([<span class="string">'Class change : '</span> num2str(class_change)])
</pre><pre class="codeoutput">Absolute change in prediction error : 4.41
Class change : 4.51
</pre><p><a id="plot"></a></p><h2>plot<a name="45"></a></h2><p>Display the kriging <tt>kr</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>kr.plot</tt> plot the meta-model <tt>kr</tt></li><li><tt>kr.plot(param,value)</tt> use set of parameters <tt>param</tt> and values <tt>value</tt> (<i>c.f.</i>, <a href="#params_plot">parameter table</a>)</li><li><tt>h=kr.plot(...)</tt> returns graphical handles</li></ul></div><p><h3>Parameters</h3>
<table id="params_plot" style="border: none;width=100%">
  <tr>
    <th><tt>param</tt></th>
    <th><tt>value</tt></th>
    <th>Description</th>
  </tr>
  <tr>
    <td><span class="string">'new_fig'</span></td>
    <td>logical, {<tt>false</tt>}</td>
    <td>Create a new figure</td>
  </tr>
  <tr>
    <td><span class="string">'lb'</span></td>
    <td>numeric, {<tt>kr.lb_x</tt>}</td>
    <td>Lower bound of plot</td>
  </tr>
  <tr>
    <td><span class="string">'ub'</span></td>
    <td>numeric, {<tt>kr.ub_x</tt>}</td>
    <td>Upper bound of plot</td>
  </tr>
  <tr>
    <td><span class="string">'samples'</span></td>
    <td>logical, {<tt>true</tt>}</td>
    <td>Plot training samples</td>
  </tr>
  <tr>
    <td><span class="string">'lsty'</span></td>
    <td>string, {<span class="string">'k-'</span>}</td>
    <td>Line style (1D), see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a></td>
  </tr>
  <tr>
    <td><span class="string">'psty'</span></td>
    <td>string, {<span class="string">'ko'</span>}</td>
    <td>Samples style, see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a></td>
  </tr>
  <tr>
    <td><span class="string">'legend'</span></td>
    <td>logical, {<tt>true</tt>}</td>
    <td>Add legend</td>
  </tr>
  <tr>
    <td><span class="string">'CI'</span></td>
    <td>logical, {<tt>true</tt>}</td>
    <td>Display confidence interval on the predictor.</td>
  </tr>
  <tr>
    <td><span class="string">'alpha'</span></td>
    <td>numeric, {0.05}</td>
    <td>Significance level for (1-<tt>alpha</tt>) confidence interval.</td>
  </tr>
  <tr>
    <td><span class="string">'CI_color'</span></td>
    <td>string or rgb color, {<span class="string">'k'</span>}</td>
    <td>Color of the confidence interval.</td>
  </tr>
  <tr>
    <td><span class="string">'CI_alpha'</span></td>
    <td>numeric, {<span class="string">0.3</span>}</td>
    <td>Alpha (transparence) level of the conficende interval.</td>
  </tr>
</table></p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x-4;
x=[2;8];y=f(x);
kr=CODES.fit.kriging(x,y);
kr.plot(<span class="string">'new_fig'</span>,true)
</pre><img vspace="5" hspace="5" src="kriging_method_01.png" alt=""> <p><h3>See also</h3><a href=#isoplot>isoplot</a></p><p><a id="isoplot"></a></p><h2>isoplot<a name="47"></a></h2><p>Display the 0 isocontour of the meta-model <tt>kr</tt></p><p><h3>Syntax</h3></p><div><ul><li><tt>kr.isoplot</tt> plot the 0 isocontour of the meta-model <tt>kr</tt></li><li><tt>kr.isoplot(param,value)</tt> use set of parameters <tt>param</tt> and values <tt>value</tt> (<i>c.f.</i>, <a href="#params_isoplot">parameter table</a>)</li><li><tt>h=kr.isoplot(...)</tt> returns graphical handles</li></ul></div><p><h3>Parameters</h3>
<table id="params_isoplot" style="border: none;width=100%">
  <tr>
    <th><tt>param</tt></th>
    <th><tt>value</tt></th>
    <th>Description</th>
  </tr>
  <tr>
    <td><span class="string">'new_fig'</span></td>
    <td>logical, {<tt>false</tt>}</td>
    <td>Create a new figure.</td>
  </tr>
  <tr>
    <td><span class="string">'th'</span></td>
    <td>numeric, {<tt>0</tt>}</td>
    <td>Isovalue to plot.</td>
  </tr>
  <tr>
    <td><span class="string">'lb'</span></td>
    <td>numeric, {<tt>kr.lb_x</tt>}</td>
    <td>Lower bound of plot.</td>
  </tr>
  <tr>
    <td><span class="string">'ub'</span></td>
    <td>numeric, {<tt>kr.ub_x</tt>}</td>
    <td>Upper bound of plot.</td>
  </tr>
  <tr>
    <td><span class="string">'samples'</span></td>
    <td>logical, {<tt>true</tt>}</td>
    <td>Plot training samples.</td>
  </tr>
  <tr>
    <td><span class="string">'mlsty'</span></td>
    <td>string, {<span class="string">'r-'</span>}</td>
    <td>Line style for -1 domain (1D), see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
  </tr>
  <tr>
    <td><span class="string">'plsty'</span></td>
    <td>string, {<span class="string">'b-'</span>}</td>
    <td>Line style for +1 domain (1D), see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
  </tr>
  <tr>
    <td><span class="string">'bcol'</span></td>
    <td>string, {<span class="string">'k'</span>}</td>
    <td>Boundary color, see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
  </tr>
  <tr>
    <td><span class="string">'mpsty'</span></td>
    <td>string, {<span class="string">'ro'</span>}</td>
    <td>-1 samples style, see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
  </tr>
  <tr>
    <td><span class="string">'ppsty'</span></td>
    <td>string, {<span class="string">'bo'</span>}</td>
    <td>+1 samples style, see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
  </tr>
  <tr>
    <td><span class="string">'use_light'</span></td>
    <td>logical, {<tt>true</tt>}</td>
    <td>Use light (3D).</td>
  </tr>
  <tr>
    <td><span class="string">'prev_leg'</span></td>
    <td>cell, { {} }</td>
    <td>Previous legend entry.</td>
  </tr>
  <tr>
    <td><span class="string">'legend'</span></td>
    <td>logical, {<tt>true</tt>}</td>
    <td>Add legend.</td>
  </tr>
</table></p><p><h3>Example</h3></p><pre class="codeinput">f=@(x)x-4;
x=[2;8];y=f(x);
kr=CODES.fit.kriging(x,y);
kr.isoplot(<span class="string">'new_fig'</span>,true)
</pre><img vspace="5" hspace="5" src="kriging_method_02.png" alt=""> <p><h3>See also</h3><a href=#isoplot>isoplot</a></p><p><a id="redLogLH"></a></p><h2>redLogLH<a name="50"></a></h2><p>Compute the reduced log likelihood</p><p><h3>Syntax</h3></p><div><ul><li><tt>lh=kr.redLogLH</tt> compute the reduced log likelihood <tt>lh</tt> using parameters stored in <tt>kr</tt>.</li><li><tt>lh=kr.redLogLH(theta)</tt> compute the reduced log likelihood <tt>lh</tt> using <tt>theta</tt>.</li><li><tt>lh=kr.redLogLH(theta,delta_2)</tt> compute the reduced log likelihood <tt>lh</tt> using <tt>theta</tt> and <tt>delta_2</tt>.</li></ul></div><p><h3>Example</h3></p><pre class="codeinput">kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
lh1=kr.redLogLH;
lh2=kr.redLogLH(kr.theta,kr.sigma_n_2/kr.sigma_y_2);
lh3=kr.redLogLH(1,0);
disp([lh1 lh2 lh3])
</pre><pre class="codeoutput">    1.3863    1.3863    1.0003

</pre><p><a id="plot"></a></p><p>Copyright &copy; 2015 Computational Optimal Design of Engineering Systems
(CODES) Laboratory. University of Arizona.</p><p><table style="border: none">
  <tr style="border: none">
    <td style="border: none;padding-left: 0px;">
      <a href ="http://codes.arizona.edu/"><img style="height: 50px;" src ="CODES_logo.png"></a>
    </td><td style="border: none; vertical-align: middle;padding-left: 10px;">
      <a href ="http://codes.arizona.edu/"><span style="font-weight:bold;font-family:Arial;font-size: 20px;color: #002147"><span style="color: #AB0520;">C</span>omputational <span style="color: #AB0520;">O</span>ptimal <span style="color: #AB0520;">D</span>esign of<br><span style="color: #AB0520;">E</span>ngineering <span style="color: #AB0520;">S</span>ystems</span></a>
    </td><td style="border: none;padding-right: 0px;">
      <a href = "http://www.arizona.edu/"><img style="height: 50px;" src = "AZlogo.png"></a>
    </td>
  </tr>
</table></p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% <CODES.html CODES> / <fit_main.html fit> / kriging (methods)
% _Methods of the class_ |kriging|
%
% <html>
%   <style type="text/css">
%     span.string{color:#A020F0;font-family:monospace;}
%     p{text-align: justify;}
%   </style>
% </html>
%
%%
% <html><a id="eval"></a></html>
%% eval
% Evaluate mean prediction of new samples |x|
%
% <html><h3>Syntax</h3></html>
%
% * |y_hat=kr.eval(x)| return the Kriging values |y_hat| of the samples
% |x|.
% * |[y_hat,grad]=kr.eval(x)| return the gradients |grad| at |x|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
[y_hat,grad]=kr.eval([10 0.5;5 0.6;14 0.8]);
CODES.common.disp_matrix([y_hat grad],[],{'Y','grad1','grad2'})
%%
% <html><h3>See also</h3><a href=#eval_var>eval_var</a> <a
% href=#eval_all>eval_all</a> <a href=#p_pos>P_pos</a></html>
%
% <html><a id="eval_var"></a></html>
%% eval_var
% Evaluate predicted variance at new samples |x|
%
% <html><h3>Syntax</h3></html>
%
% * |var_hat=kr.eval_var(x)| return the Kriging variance |var_hat| of the
% samples |x|.
% * |[var_hat,grad]=kr.eval_var(x)| return the gradients |grad| at |x|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
[y_hat,grad]=kr.eval_var([10 0.5;5 0.6;14 0.8]);
CODES.common.disp_matrix([y_hat grad],[],{'Y','grad1','grad2'})
%%
% <html><h3>See also</h3><a href=#eval>eval</a> <a
% href=#eval_all>eval_all</a> <a href=#p_pos>P_pos</a></html>
%
% <html><a id="eval_all"></a></html>
%% eval_all
% Evaluate predicted mean and variance at new samples |x|
%
% <html><h3>Syntax</h3></html>
%
% * |y_hat=kr.eval_var(x)| return the Kriging variance |var_hat| of the
% samples |x|.
% * |[y_hat,var_hat]=kr.eval_var(x)| return the Kriging variance |var_hat|
% of the samples |x|.
% * |[y_hat,var_hat,grad_y]=kr.eval_var(x)| return the gradients of the
% mean |grad_y| at |x|.
% * |[y_hat,var_hat,grad_y,grad_var]=kr.eval_var(x)| return the gradients
% of the variance |grad_var| at |x|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
[y_hat,var_hat,grad_y,grad_var]=kr.eval_all([10 0.5;5 0.6;14 0.8]);
CODES.common.disp_matrix([y_hat var_hat grad_y grad_var],[],...
    {'Y','Var','Y_grad1','Y_grad2','Var_grad1','Var_grad2'})
%%
% <html><h3>See also</h3><a href=#eval>eval</a> <a
% href=#eval_var>eval_var</a> <a href=#p_pos>P_pos</a></html>
%
% <html><a id="p_pos"></a></html>
%% P_pos
% Compute the probability of the kriging prediction to be higher than a
% threshold.
%
% <html><h3>Syntax</h3></html>
%
% * |p=kr.eval_var(x)| return the probability |p| of the samples |x| to be
% higher than 0.
% * |p=kr.eval_var(x,th)| return the probability |p| of the samples |x| to
% be higher than |th|.
% * |[p,grad]=kr.eval_var(...)| return the gradient |grad| of probability
% |p|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
[p,grad]=kr.P_pos([10 0.5;5 0.6;14 0.8],0.25);
CODES.common.disp_matrix([p grad],[],{'p','grad1','grad2'})
%%
% <html><a id="class"></a></html>
%% class
% Provides the sign of input |y|, different than MATLAB
% sign function for |y=0|.
%
% <html><h3>Syntax</h3></html>
%
% * |lab=kr.class(y)| computes labels |lab| for function values |y|.
%
% <html><h3>Example</h3></html>
svm=CODES.fit.svm([1;2],[1;-1]);
y=[1;-1;-2;3;0];
lab=svm.class(y);
disp([['   y : ';' lab : ';'sign : '] num2str([y';lab';sign(y')],'%1.3f  ')])
%%
% <html><h3>See also</h3><a href=#eval_class>eval_class</a></html>
%
% <html><a id="eval_class"></a></html>
%% eval_class
% Evaluate class of new samples |x|
%
% <html><h3>Syntax</h3></html>
%
% * |lab=kr.eval_class(x)| computes the labels |lab| of the input samples
% |x|.
% * |[lab,y_hat]|=kr.eval_class(x) also returns predicted function values
% |y_hat|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1;2],[1;-1]);
x=[0;1;2;3];
[lab,y_hat]=kr.eval_class(x);
disp([['y_hat : ';'  lab : '] num2str([y_hat';lab'],'%1.3f  ')])
%%
% <html><h3>See also</h3><a href=#class>class</a></html>
%
% <html><a id="scale"></a></html>
%% scale
% Perform scaling of samples |x_unsc|
%
% <html><h3>Syntax</h3></html>
%
% * |x_sc=kr.scale(x_unsc)| scales |x_unsc|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[1;-1]);
x_unsc=[1 1;10 1.5;20 2];
x_sc=kr.scale(x_unsc);
disp('        Unscaled             Scaled')
disp([x_unsc x_sc])
%%
% <html><h3>See also</h3><a href=#unscale>unscale</a></html>
%
% <html><a id="unscale"></a></html>
%% unscale
% Perform unscaling of samples |x_sc|
%
% <html><h3>Syntax</h3></html>
%
% * |x_unsc=kr.unscale(x_sc)| unscales |x_sc|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[1;-1]);
x_sc=[0 0;0.4737 -1;1 1];
x_unsc=kr.unscale(x_sc);
disp('         Scaled             Unscaled')
disp([x_sc x_unsc])
%%
% <html><h3>See also</h3><a href=#scale>scale</a></html>
%
% <html><a id="scale_y"></a></html>
%% scale_y
% Perform scaling of function values |y_unsc|
%
% <html><h3>Syntax</h3></html>
%
% * |y_sc=kr.scale_y(y_unsc)| scales |y_unsc|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[1;-3]);
y_unsc=[0.5;1;2];
y_sc=kr.scale_y(y_unsc);
CODES.common.disp_matrix([y_unsc y_sc],[],{'Unscaled','Scaled'})
%%
% <html><h3>See also</h3><a href=#unscale_y>unscale_y</a></html>
%
% <html><a id="unscale_y"></a></html>
%% unscale_y
% Perform unscaling of function values |y_sc|
%
% <html><h3>Syntax</h3></html>
%
% * |y_unsc=kr.unscale_y(y_sc)| unscales |y_sc|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[1;-1]);
y_sc=[0;0.25;0.75];
y_unsc=kr.unscale_y(y_sc);
CODES.common.disp_matrix([y_unsc y_sc],[],{'Unscaled','Scaled'})
%%
% <html><h3>See also</h3><a href=#scale>scale</a></html>
%%
% <html><a id="add"></a></html>
%% add
% Retrain |kr| after adding a new sample |(x,y)|
%
% <html><h3>Syntax</h3></html>
%
% * |kr=kr.add(x,y)| adds a new sample |x| with function value |y|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1;2],[1;-1]);
disp(['Predicted class at x=1.4, ' num2str(kr.eval_class(1.4))])
kr=kr.add(1.5,-1);
disp(['Updated predicted class at x=1.4, ' num2str(kr.eval_class(1.4))])
%%
% <html><a id="mse"></a></html>
%% mse
% Compute the Mean Square Error (MSE) for |(x,y)| (not for classification)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.mse(x,y)| computes the MSE for the samples |(x,y)|.
%
% <html><h3>Description</h3></html>
%
% For a representative (sample,label) $(\mathbf{x},\mathbf{y})$ of the
% domain of interest and predicted values $\tilde{\mathbf{y}}$, the
% Mean Square Error is defined as:
%
% $$mse=\frac{1}{n}\sum_{i=1}^n\left(y^{(i)}-\tilde{y}^{(i)}\right)^2$$
%
% <html><h3>Example</h3></html>
f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.mse(x_t,y_t);
disp('Mean Square Error:')
disp(err)
%%
% <html><h3>See also</h3><a href=#auc>auc</a> | <a
% href=#me>me</a> | <a href=#rmse>rmse</a> | <a
% href=#rmae>rmae</a> | <a href=#r2>r2</a> | <a
% href=#cv>cv</a> | <a href=#loo>loo</a></html>
%
% <html><a id="rmse"></a></html>
%% rmse
% Compute the Root Mean Square Error (RMSE) for |(x,y)| (not for
% classification)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.rmse(x,y)| computes the RMSE for the samples |(x,y)|.
%
% <html><h3>Description</h3></html>
%
% For a representative (sample,label) $(\mathbf{x},\mathbf{y})$ of the
% domain of interest and predicted values $\tilde{\mathbf{y}}$, the
% Root Mean Square Error is defined as:
%
% $$rmse=\sqrt{\frac{1}{n}\sum_{i=1}^n\left(y^{(i)}-\tilde{y}^{(i)}\right)^2}$$
%
% <html><h3>Example</h3></html>
f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.rmse(x_t,y_t);
disp('Root Mean Square Error:')
disp(err)
%%
% <html><h3>See also</h3><a href=#auc>auc</a> | <a
% href=#me>me</a> | <a href=#mse>mse</a> | <a
% href=#rmae>rmae</a> | <a href=#r2>r2</a></html>
%
% <html><a id="nmse"></a></html>
%% nmse
% Compute the Normalized Mean Square Error (NMSE) (%) for |(x,y)| (not for
% classification)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.nmse(x,y)| computes the NMSE for the samples |(x,y)|.
%
% <html><h3>Description</h3></html>
%
% For a representative (sample,label) $(\mathbf{x},\mathbf{y})$ of the
% domain of interest and predicted values $\tilde{\mathbf{y}}$, the
% Normalized Mean Square Error is defined as:
%
% $$nmse=\frac{\sum_{i=1}^n\left(y^{(i)}-\tilde{y}^{(i)}\right)^2}{\sum_{i=1}^n\left(y^{(i)}-\bar{\mathbf{y}}\right)^2}$$
%
% where $\bar{\mathbf{y}}$ is the average of the training values
% $\mathbf{y}$.
%
% <html><h3>Example</h3></html>
f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.nmse(x_t,y_t);
disp('Normalized Mean Square Error:')
disp([num2str(err,'%5.2f') ' %'])
%%
% <html><h3>See also</h3><a href=#auc>auc</a> | <a
% href=#me>me</a> | <a href=#mse>mse</a> | <a href=#rmse>rmse</a> | <a
% href=#rmae>rmae</a> | <a href=#r2>r2</a></html>
%
% <html><a id="rmae"></a></html>
%% rmae
% Compute the Relative Maximum Absolute Error (RMAE) for |(x,y)| (not for
% classification)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.rmae(x,y)| computes the RMAE for the samples |(x,y)|.
%
% <html><h3>Description</h3></html>
%
% For a representative (sample,label) $(\mathbf{x},\mathbf{y})$ of the
% domain of interest and predicted values $\tilde{\mathbf{y}}$, the
% Relative Maximum Absolute Error is defined as:
%
% $$rmae=\frac{\mathop{\max}\limits_{i}\ \left|y^{(i)}-\tilde{y}^{(i)}\right|}{\sigma_\mathbf{y}}$$
%
% where $\sigma_\mathbf{y}$ is the standard deviation of the training
% values $\mathbf{y}$:
%
% $$\sigma_\mathbf{y}=\sqrt{\frac{1}{n-1}\sum_{i=1}^n\left(y^{(i)}-\bar{\mathbf{y}}\right)^2}$$
%
% where $\bar{\mathbf{y}}$ is the average of the training values
% $\mathbf{y}$.
%
% <html><h3>Example</h3></html>
f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.rmae(x_t,y_t);
disp('Relative Maximum Absolute Error:')
disp(err)
%%
% <html><h3>See also</h3><a href=#auc>auc</a> | <a
% href=#me>me</a> | <a href=#mse>mse</a> | <a href=#rmse>rmse</a> | <a
% href=#r2>r2</a></html>
%
% <html><a id="r2"></a></html>
%% r2
% Compute the coefficient of determination (R squared) for |(x,y)| (not for
% classification)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.r2(x,y)| computes the R squared for the samples |(x,y)|
% * |[stat,TSS]=kr.r2(x,y)| return the Total Sum of Squares |TSS|
% * |[stat,TSS,RSS]=kr.r2(x,y)| returns the Residual Sum of Squares
% |RSS|
%
% <html><h3>Description</h3></html>
%
% For a representative (sample,label) $(\mathbf{x},\mathbf{y})$ of the
% domain of interest and predicted values $\tilde{\mathbf{y}}$, the
% coefficient of determination is defined as:
%
% $$r2=1-\frac{RSS}{TSS}$$
%
% where:
%
% $$RSS=\sum_{i=1}^n\left(y^{(i)}-\tilde{y}^{(i)}\right)^2$$
%
% $$TSS=\sum_{i=1}^n\left(\tilde{y}^{(i)}-\bar{\mathbf{y}}\right)^2$$
%
% where $\bar{\mathbf{y}}$ is the average of the training values
% $\mathbf{y}$.
%
% <html><h3>Example</h3></html>
f=@(x)x.*sin(x);
x=linspace(0,10,5)';y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.r2(x_t,y_t);
disp('Coefficient of determination:')
disp(err)
%%
% <html><h3>See also</h3><a href=#auc>auc</a> | <a
% href=#me>me</a> | <a href=#mse>mse</a> | <a href=#rmse>rmse</a> | <a
% href=#rmae>rmae</a></html>
%
% <html><a id="me"></a></html>
%% me
% Compute the Misclassification Error (ME) for |(x,y)| (%)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.me(x,y)| compute the me for |(x,y)|
% * |stat=kr.me(x,y,use_balanced)| returns Balanced Misclassification
% Error (BME) if use_balanced is set to true
%
% <html><h3>Description</h3></html>
%
% For a representative (sample,label) $(\mathbf{x},\mathbf{y})$ of the
% domain of interest and predicted labels $\tilde{\mathbf{y}}$, the
% classification error is defined as:
%
% $$err_{class}=\frac{1}{n}\sum_{i=1}^n\mathcal{I}_{y^{(i)}\neq\tilde{y}^{(i)}}$$
%
% On the other hand, the balanced classification error is defined as:
%
% $$err_{class}^{bal}=\frac{1}{n}\sum_{i=1}^n\left[w_p\mathcal{I}_{y^{(i)}=+1}\mathcal{I}_{y^{(i)}\neq\tilde{y}^{(i)}}+w_m\mathcal{I}_{y^{(i)}=-1}\mathcal{I}_{y^{(i)}\neq\tilde{y}^{(i)}}\right]$$
%
% where $w_p$ and $w_m$ are weights computed based on training samples such
% that:
%
% $$w_p=\frac{n}{2n_+}\quad;\quad w_m=\frac{n}{2n_-}$$
%
% where $n$ is the total number of samples and $n_+$ (resp. $n_-$) is the
% total number of positive (resp. negative) samples. This weights satisfy a
% set of condition:
%
% * $err_{class}^{bal}=0.5$ if all positive or all negative samples are
% misclassified;
% * $err_{class}^{bal}=0$ if all samples are properly classified;
% * $err_{class}^{bal}=1$ if all samples are misclassified;
% * $w_p=w_m=1\textrm{ if }n_+=n_-=\frac{n}{2}\quad\Rightarrow\quad err_{class}^{bal}=err_{class}$.
%
% This function is typically used to validate meta-models on an independent
% validation set as in <#ref_peng Jiang and Missoum (2014)>.
%
% <html><h3>Example</h3></html>
f=@(x)x-4;
x=[2;8];y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.me(x_t,y_t);
bal_err=kr.me(x_t,y_t,true);
disp('On evenly balanced training set, standard and balanced prediction error return same values')
disp([err bal_err])
x=[2;5;8];y=f(x);
kr=CODES.fit.kriging(x,y);
err=kr.me(x_t,y_t);
bal_err=kr.me(x_t,y_t,true);
disp('On unevenly balanced training set, standard and balanced prediction error return different values')
disp([err bal_err])

%%
% <html><h3>See also</h3><a href=#auc>auc</a> | <a href=#mse>mse</a> | <a
% href=#rmse>rmse</a> | <a href=#cv>cv</a> | <a href=#loo>loo</a></html>
%
% <html><a id="auc"></a></html>
%% auc
% Returns the Area Under the Curve (AUC) for (x,y) (%)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=CODES.fit.kr.auc(x,y)| return the AUC |stat| for the samples
% |(x,y)|
% * |stat=CODES.fit.kr.auc(x,y,ROC)| plot the ROC curves if |ROC|is set
% to |true|
% * |[stat,FP,TP]=CODES.fit.kr.auc(...)| returns the false positive rate
% |FP| and the true positive rate |TP|
%
% <html><h3>Description</h3></html>
%
% A receiver operating characteristic (ROC) curve <#ref_metz Metz (1978)>
% is a graphical representation of the relation between true and false
% positive predictions for a binary classifier. It uses all possible
% decision thresholds from the prediction. In the case of kr
% classification, thresholds are defined by the kr values. More
% specifically, for each threshold a True Positive Rate:
%
% $$TPR=\frac{TP}{TP+FN}$$
%
% and a False Positive Rate:
%
% $$FPR=\frac{FP}{FP+TN}$$
%
% are calculated. $TP$ and $TN$ are the number of true positive and true
% negative predictions while $FP$ and $FN$ are the number of false positive
% and false negative predictions, respectively. The ROC curve represents
% $TPR$ as a function of $FPR$.
%
% Once the ROC curve is constructed, the area under the ROC curve (AUC) can
% be calculated and used as a validation metric. A perfect classifier will
% have an AUC equal to one. An AUC value of 0.5 indicates no discriminative
% ability.
%
% <html><img src='rocexample.png' width=500></html>
%
% <html><h3>Example</h3></html>
f=@(x)x(:,2)-sin(10*x(:,1))/4-0.5;
x=CODES.sampling.cvt(30,2);y=f(x);
kr=CODES.fit.kriging(x,y);
auc_val=kr.auc(kr.X,kr.Y);
x_t=rand(1000,2);
y_t=f(x_t);
auc_new=kr.auc(x_t,y_t);
disp(['AUC value over training set : ' num2str(auc_val,'%7.3f')])
disp([' AUC value over testing set : ' num2str(auc_new,'%7.3f')])
%%
% <html><h3>See also</h3><a href=#me>me</a> | <a
% href=#mse>mse</a> | <a href=#rmse>rmse</a> | <a href=#cv>cv</a></html>
%
% <html><a id="loo"></a></html>
%% loo
% Returns the Leave One Out (LOO) error (%)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.loo| return the loo error |stat|
% * |stat=kr.loo(param,value)| use set of parameters |param| and values
% |value| (_c.f._, <#params_loo parameter table>)
%
% <html><h3>Description</h3></html>
%
% Define $\tilde{l}^{(i)}$ the $i\textrm{\textsuperscript{th}}$ predicted
% label and $\tilde{l}_{-j}^{(i)}$ the $i\textrm{\textsuperscript{th}}$
% predicted label using the kr trained without the
% $j\textrm{\textsuperscript{th}}$ sample. The Leave One Out (LOO) error is
% defined as:
%
% $$err_{loo}=\frac{1}{n}\sum_{i=1}^n\mathcal{I}_{\tilde{l}^{(i)}\neq\tilde{l}_{-i}^{(i)}}$$
%
% On the other hand, the balanced LOO error is defined as: 
%
% $$err_{loo}^{bal}=\frac{1}{n}\sum_{i=1}^n\left[w_p\mathcal{I}_{l^{(i)}=+1}\mathcal{I}_{\tilde{l}^{(i)}\neq\tilde{l}_{-i}^{(i)}}+w_m\mathcal{I}_{l^{(i)}=-1}\mathcal{I}_{\tilde{l}^{(i)}\neq\tilde{l}_{-i}^{(i)}}\right]$$
%
% $w_p$ and $w_m$ are weights computed based on
% training samples such that:
%
% $$w_p=\frac{n}{2n_+}\quad;\quad w_m=\frac{n}{2n_-}$$
%
% where $n$ is the total number of samples and $n_+$ (resp. $n_-$) is the
% total number of positive (resp. negative) samples. This weights satisfy a
% set of condition:
%
% * $err_{loo}^{bal}=0.5$ if all positive or all negative samples are
% misclassified;
% * $err_{loo}^{bal}=0$ if no samples are misclassified;
% * $err_{loo}^{bal}=1$ if all samples are miscalssified;
% * $w_p=w_m=1\textrm{ if }n_+=n_-=\frac{n}{2}\quad\Rightarrow\quad err_{loo}^{bal}=err_{loo}$.
%
% <html><h3>Parameters</h3>
% <table id="params_loo" style="border: none;width=100%">
%   <tr>
%     <th><tt>param</tt></th>
%     <th><tt>value</tt></th>
%     <th>Description</th>
%   </tr>
%   <tr>
%     <td><span class="string">'use_balanced'</span></td>
%     <td>logical, {<tt>false</tt>}</td>
%     <td>Only for Misclassification Error, uses Balanced Misclassification Error if <tt> true></tt></td>
%   </tr>
%   <tr>
%     <td><span class="string">'metric'</span></td>
%     <td>{<span class="string">'me'</span>}, <span class="string">'mse'</span></td>
%     <td>Metric on which LOO procedure is applied, Misclassification Error (<span class="string">'me'</span>) or Mean Square Error (<span class="string">'mse'</span>)</td>
%   </tr>
% </table></html>
%
% <html><h3>Example</h3></html>
f=@(x)x(:,2)-sin(10*x(:,1))/4-0.5;
x=CODES.sampling.cvt(6,2);y=f(x);
kr=CODES.fit.kriging(x,y);
loo_err=kr.loo;
bal_loo_err=kr.loo('use_balanced',true);
disp('On evenly balanced training set, standard and balanced loo error return same values')
disp([loo_err bal_loo_err])
x=CODES.sampling.cvt(5,2);y=f(x);
kr=CODES.fit.kriging(x,y);
loo_err=kr.loo;
bal_loo_err=kr.loo('use_balanced',true);
disp('On unevenly balanced training set, standard and balanced loo error return different values')
disp([loo_err bal_loo_err])
%%
% <html><h3>See also</h3><a href=#cv>cv</a> | <a href=#auc>auc</a> | <a
% href=#me>me</a> | <a href=#mse>mse</a></html>
%
% <html><a id="cv"></a></html>
%% cv
% Returns the Cross Validation (CV) error over 10 folds (%)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.cv| return the cv error |stat|
% * |stat=kr.cv(param,value)| use set of parameters |param| and values
% |value| (_c.f._, <#params_cv parameter table>)
%
% <html><h3>Description</h3></html>
%
% This function follow the same outline as the <#loo |loo|> but uses a 10
% fold CV procedure instead of an $n$ fold one. Therefore, the |cv| and
% |loo| are the same for $n\leq10$ and |cv| returns an estimate of |loo|
% that is faster to compute for $n>10$.
%
% <html><h3>Parameters</h3>
% <table id="params_cv" style="border: none;width=100%">
%   <tr>
%     <th><tt>param</tt></th>
%     <th><tt>value</tt></th>
%     <th>Description</th>
%   </tr>
%   <tr>
%     <td><span class="string">'use_balanced'</span></td>
%     <td>logical, {<tt>false</tt>}</td>
%     <td>Only for Misclassification Error, uses Balanced Misclassification Error if <tt> true></tt></td>
%   </tr>
%   <tr>
%     <td><span class="string">'metric'</span></td>
%     <td><span class="string">'auc'</span>, {<span class="string">'me'</span>}, <span class="string">'mse'</span></td>
%     <td>Metric on which CV procedure is applied: Area Under the Curve (<span class="string">'auc'</span>), Misclassification Error (<span class="string">'me'</span>) or Mean Square Error (<span class="string">'mse'</span>)</td>
%   </tr>
% </table></html>
%
% <html><h3>Example</h3></html>
f=@(x)x(:,1)-0.5;
x=CODES.sampling.cvt(30,2);y=f(x);
kr=CODES.fit.kriging(x,y);
rng(0); % To ensure same CV folds
cv_err=kr.cv;
rng(0);
bal_cv_err=kr.cv('use_balanced',true);
disp('On evenly balanced training set, standard and balanced cv error return same values')
disp([cv_err bal_cv_err])
x=CODES.sampling.cvt(31,2);y=f(x);
kr=CODES.fit.kriging(x,y);
rng(0);
cv_err=kr.cv;
rng(0);
bal_cv_err=kr.cv('use_balanced',true);
disp('On unevenly balanced training set, standard and balanced cv error return different values')
disp([cv_err bal_cv_err])
%%
% <html><h3>See also</h3><a href=#loo>loo</a> | <a href=#auc>auc</a> | <a
% href=#me>me</a> | <a href=#mse>mse</a></html>
%
% <html><a id="class_change"></a></html>
%% class_change
% Compute the change of class between two meta-models over a sample |x| (%)
%
% <html><h3>Syntax</h3></html>
%
% * |stat=kr.class_change(kr_old,x)| compute the change of class of the
% sample |x| from meta-model |kr_old| to meta-model |kr| 
%
% <html><h3>Description</h3></html>
%
% This metric was used in <#ref_bas Basudhar and Missoum (2008)> as
% convergence metric and is defined as:
%
% $$class_{change}=\frac{1}{n_c}\sum_{i=1}^{n_c}\mathcal{I}_{\hat{y}_c^{(i)}\neq\tilde{y}_c^{(i)}}$$
%
% where $n_c$ is the number of convergence samples and $\hat{y}_c^{(i)}$
% (resp. $\tilde{y}_c^{(i)}$) is the $i\textrm{\textsuperscript{th}}$
% convergence predicted label using |kr_old| (resp. |kr|).
%
% <html><h3>Example</h3></html>
f=@(x)x-4;
x=[2;8];y=f(x);
kr=CODES.fit.kriging(x,y);
x_t=linspace(0,10,1e4)';y_t=f(x_t);
err=kr.me(x_t,y_t);
kr_new=kr.add(5,f(5));
err_new=kr_new.me(x_t,y_t);
class_change=kr_new.class_change(kr,x_t);
disp(['Absolute change in prediction error : ' num2str(abs(err_new-err))])
disp(['Class change : ' num2str(class_change)])
%%
% <html><a id="plot"></a></html>
%% plot
% Display the kriging |kr|
%
% <html><h3>Syntax</h3></html>
%
% * |kr.plot| plot the meta-model |kr|
% * |kr.plot(param,value)| use set of parameters |param| and values
% |value| (_c.f._, <#params_plot parameter table>)
% * |h=kr.plot(...)| returns graphical handles
%
% <html><h3>Parameters</h3>
% <table id="params_plot" style="border: none;width=100%">
%   <tr>
%     <th><tt>param</tt></th>
%     <th><tt>value</tt></th>
%     <th>Description</th>
%   </tr>
%   <tr>
%     <td><span class="string">'new_fig'</span></td>
%     <td>logical, {<tt>false</tt>}</td>
%     <td>Create a new figure</td>
%   </tr>
%   <tr>
%     <td><span class="string">'lb'</span></td>
%     <td>numeric, {<tt>kr.lb_x</tt>}</td>
%     <td>Lower bound of plot</td>
%   </tr>
%   <tr>
%     <td><span class="string">'ub'</span></td>
%     <td>numeric, {<tt>kr.ub_x</tt>}</td>
%     <td>Upper bound of plot</td>
%   </tr>
%   <tr>
%     <td><span class="string">'samples'</span></td>
%     <td>logical, {<tt>true</tt>}</td>
%     <td>Plot training samples</td>
%   </tr>
%   <tr>
%     <td><span class="string">'lsty'</span></td>
%     <td>string, {<span class="string">'k-'</span>}</td>
%     <td>Line style (1D), see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a></td>
%   </tr>
%   <tr>
%     <td><span class="string">'psty'</span></td>
%     <td>string, {<span class="string">'ko'</span>}</td>
%     <td>Samples style, see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a></td>
%   </tr>
%   <tr>
%     <td><span class="string">'legend'</span></td>
%     <td>logical, {<tt>true</tt>}</td>
%     <td>Add legend</td>
%   </tr>
%   <tr>
%     <td><span class="string">'CI'</span></td>
%     <td>logical, {<tt>true</tt>}</td>
%     <td>Display confidence interval on the predictor.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'alpha'</span></td>
%     <td>numeric, {0.05}</td>
%     <td>Significance level for (1-<tt>alpha</tt>) confidence interval.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'CI_color'</span></td>
%     <td>string or rgb color, {<span class="string">'k'</span>}</td>
%     <td>Color of the confidence interval.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'CI_alpha'</span></td>
%     <td>numeric, {<span class="string">0.3</span>}</td>
%     <td>Alpha (transparence) level of the conficende interval.</td>
%   </tr>
% </table></html>
%
% <html><h3>Example</h3></html>
f=@(x)x-4;
x=[2;8];y=f(x);
kr=CODES.fit.kriging(x,y);
kr.plot('new_fig',true)
%%
% <html><h3>See also</h3><a href=#isoplot>isoplot</a></html>
%
% <html><a id="isoplot"></a></html>
%% isoplot
% Display the 0 isocontour of the meta-model |kr|
%
% <html><h3>Syntax</h3></html>
%
% * |kr.isoplot| plot the 0 isocontour of the meta-model |kr|
% * |kr.isoplot(param,value)| use set of parameters |param| and values
% |value| (_c.f._, <#params_isoplot parameter table>)
% * |h=kr.isoplot(...)| returns graphical handles
%
% <html><h3>Parameters</h3>
% <table id="params_isoplot" style="border: none;width=100%">
%   <tr>
%     <th><tt>param</tt></th>
%     <th><tt>value</tt></th>
%     <th>Description</th>
%   </tr>
%   <tr>
%     <td><span class="string">'new_fig'</span></td>
%     <td>logical, {<tt>false</tt>}</td>
%     <td>Create a new figure.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'th'</span></td>
%     <td>numeric, {<tt>0</tt>}</td>
%     <td>Isovalue to plot.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'lb'</span></td>
%     <td>numeric, {<tt>kr.lb_x</tt>}</td>
%     <td>Lower bound of plot.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'ub'</span></td>
%     <td>numeric, {<tt>kr.ub_x</tt>}</td>
%     <td>Upper bound of plot.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'samples'</span></td>
%     <td>logical, {<tt>true</tt>}</td>
%     <td>Plot training samples.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'mlsty'</span></td>
%     <td>string, {<span class="string">'r-'</span>}</td>
%     <td>Line style for -1 domain (1D), see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'plsty'</span></td>
%     <td>string, {<span class="string">'b-'</span>}</td>
%     <td>Line style for +1 domain (1D), see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'bcol'</span></td>
%     <td>string, {<span class="string">'k'</span>}</td>
%     <td>Boundary color, see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'mpsty'</span></td>
%     <td>string, {<span class="string">'ro'</span>}</td>
%     <td>-1 samples style, see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'ppsty'</span></td>
%     <td>string, {<span class="string">'bo'</span>}</td>
%     <td>+1 samples style, see also <a href="http://www.mathworks.com/help/matlab/ref/linespec.html">LineSpec</a>.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'use_light'</span></td>
%     <td>logical, {<tt>true</tt>}</td>
%     <td>Use light (3D).</td>
%   </tr>
%   <tr>
%     <td><span class="string">'prev_leg'</span></td>
%     <td>cell, { {} }</td>
%     <td>Previous legend entry.</td>
%   </tr>
%   <tr>
%     <td><span class="string">'legend'</span></td>
%     <td>logical, {<tt>true</tt>}</td>
%     <td>Add legend.</td>
%   </tr>
% </table></html>
%
% <html><h3>Example</h3></html>
f=@(x)x-4;
x=[2;8];y=f(x);
kr=CODES.fit.kriging(x,y);
kr.isoplot('new_fig',true)
%%
% <html><h3>See also</h3><a href=#isoplot>isoplot</a></html>
%%
% <html><a id="redLogLH"></a></html>
%% redLogLH
% Compute the reduced log likelihood
%
% <html><h3>Syntax</h3></html>
%
% * |lh=kr.redLogLH| compute the reduced log likelihood |lh| using
% parameters stored in |kr|.
% * |lh=kr.redLogLH(theta)| compute the reduced log likelihood |lh| using
% |theta|.
% * |lh=kr.redLogLH(theta,delta_2)| compute the reduced log likelihood |lh|
% using |theta| and |delta_2|.
%
% <html><h3>Example</h3></html>
kr=CODES.fit.kriging([1 1;20 2],[2;-1]);
lh1=kr.redLogLH;
lh2=kr.redLogLH(kr.theta,kr.sigma_n_2/kr.sigma_y_2);
lh3=kr.redLogLH(1,0);
disp([lh1 lh2 lh3])
%%
% <html><a id="plot"></a></html>
%%
%%
% <html>Copyright &copy; 2015 Computational Optimal Design of Engineering Systems
% (CODES) Laboratory. University of Arizona.</html>
%%
%
% <html><table style="border: none">
%   <tr style="border: none">
%     <td style="border: none;padding-left: 0px;">
%       <a href ="http://codes.arizona.edu/"><img style="height: 50px;" src ="CODES_logo.png"></a>
%     </td><td style="border: none; vertical-align: middle;padding-left: 10px;">
%       <a href ="http://codes.arizona.edu/"><span style="font-weight:bold;font-family:Arial;font-size: 20px;color: #002147"><span style="color: #AB0520;">C</span>omputational <span style="color: #AB0520;">O</span>ptimal <span style="color: #AB0520;">D</span>esign of<br><span style="color: #AB0520;">E</span>ngineering <span style="color: #AB0520;">S</span>ystems</span></a>
%     </td><td style="border: none;padding-right: 0px;">
%       <a href = "http://www.arizona.edu/"><img style="height: 50px;" src = "AZlogo.png"></a>
%     </td>
%   </tr>
% </table></html>

##### SOURCE END #####
--></body></html>